{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yk43mughLeTelvLXReyA6MzNYh15D9wI",
      "authorship_tag": "ABX9TyNm37N9Vi0EoQ6vMOj0ucgv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reganmeloche/mrpc_paraphrase/blob/main/bert_approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT-Based Approach"
      ],
      "metadata": {
        "id": "2f4iQOLHY3a-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will use a BERT-based approach on the paraphrase identification task. This approach borrows heavily from this [article](https://medium.com/mlearning-ai/nlp-day-26-semantic-similarity-with-bert-and-huggingface-transformers-ce76011d5a51), which applies some BERT techniques for preprocessing a different dataset. We will use BERT for the entire training and prediction processes. Since we are using BERT for this approach, it is strongly recommended to upgrade to a CoLab tier that allows you to use a GPU, as processing will be much much faster."
      ],
      "metadata": {
        "id": "xn8Kg4syY6gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "5it5jKq-Z7Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import transformers\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "INu0TFrnal1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "NhCHN_Tna0Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "EZ94ZW7kZqIu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etZvX6oeYg_F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "ROOT_PATH = '/content/drive/MyDrive/Colab Notebooks/NLP/ms_paraphrase'\n",
        "\n",
        "data_path = f'{ROOT_PATH}/data'\n",
        "\n",
        "train_df = pd.read_csv(f'{data_path}/train_df.csv')\n",
        "test_df = pd.read_csv(f'{data_path}/test_df.csv')\n",
        "\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_df(df):\n",
        "    s1_list = df['s1'].values\n",
        "    s2_list = df['s2'].values\n",
        "    labels = df['label'].values\n",
        "    sentence_pairs = list(zip(s1_list, s2_list))\n",
        "    return sentence_pairs, labels"
      ],
      "metadata": {
        "id": "ydJv9-tYbQvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One important note is that BERT will require us to also use a validation set for training. We will split our test set in half and use one half as the validation set"
      ],
      "metadata": {
        "id": "nuxLFHy9iSk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "val_df, test_df = train_test_split(test_df, test_size=0.5)"
      ],
      "metadata": {
        "id": "4mOgHIMpiRU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = format_df(train_df) \n",
        "\n",
        "print(X[0])"
      ],
      "metadata": {
        "id": "Qbvnk-eYbRSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "mariDNQLaF4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For many NLP tasks, BERT takes 3 pieces of input related to the natural language text input:\n",
        "\n",
        "- **The input ids:** Each token in the NL text is associated to a numerical id from a vector embedding. For each sentence pair, we will get an array of values where the text is replaced by the corresponding id. We will concatenate each of the sentence pairs for our input.\n",
        "- **The token type ids:** Since we've concatenated our input pairs, we need to give BERT a way of knowing where the first sentence ends and the second begins. This input will be an array of 0s representing the tokens in the first sentence, followed by an array of 1s representing the tokens in the second sentence. \n",
        "- **The mask ids:** All of our input vectors must be the same length, but not all of our sentence pairs are the same length. This input vector gives BERT information on the length of each sentence pair. It will consist of 1s that represent the tokens of the input text followed by 0s to fill up the rest of the vector, so that all vectors are the same length.\n",
        "\n",
        "The tokenizer that we get from HuggingFace provides powerful functionality that allows us to easily extract this information"
      ],
      "metadata": {
        "id": "U-DiPQpvaHrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inputs(sentence_pairs, tokenizer, max_len):\n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "        sentence_pairs, \n",
        "        add_special_tokens=True, \n",
        "        max_length = max_len,\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=True,\n",
        "        padding='max_length',\n",
        "    )\n",
        "\n",
        "    input_ids = np.array(encoded['input_ids'], dtype='int32')\n",
        "    attention_masks = np.array(encoded['attention_mask'], dtype='int32')\n",
        "    token_type_ids = np.array(encoded['token_type_ids'], dtype='int32')\n",
        "\n",
        "    return input_ids, token_type_ids, attention_masks"
      ],
      "metadata": {
        "id": "bhXaVDXqaHHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOKENIZER = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\", do_lower_case=True)\n",
        "MAX_LEN = 150"
      ],
      "metadata": {
        "id": "apskvDuUa2h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, token_type_ids, attention_masks = get_inputs(X, TOKENIZER, MAX_LEN)"
      ],
      "metadata": {
        "id": "RrbKYsYXa-_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a minute to analyze each of these inputs to understand how they relate to the input text. Recall that the sentence pair is first concatenated before these outputs are produced\n",
        "- **Input Ids:** Each entry represents a numeric id for the corresponding NL token. This will always start and end with some standardized ids (e.g. 101, 1012, 102). By aligning the input_ids with the original text, you can match up which number corresponds to NL token. Once the text runs out, the rest of the vector consists of 0s\n",
        "\n",
        "- **Token Type Ids:** 0s are used to represent the first sentence, 1s are for the second. Once the second sentence runs out, the rest is followed by 0s\n",
        "\n",
        "- **Attention Masks:** 1s are used to represent tokens for both sentences. This is followed by 0s. \n"
      ],
      "metadata": {
        "id": "bts4mN55droo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0])\n",
        "\n",
        "print('\\nInput Ids:', input_ids[0])\n",
        "\n",
        "print('\\nToken type Ids:', token_type_ids[0])\n",
        "\n",
        "print('\\nAttention Mask:', attention_masks[0])"
      ],
      "metadata": {
        "id": "aP9ybuBtbn8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batches"
      ],
      "metadata": {
        "id": "5pH0TNfLfdKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT operates on *batches* of input - we must therefore separate all of our input data into equal-sized batches.\n",
        "\n",
        "We have 4076 training instances. So if we operate on batches of size 8, then we should have 4076/8 ~= 510 batches\n"
      ],
      "metadata": {
        "id": "-ckS8bkjfefr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8"
      ],
      "metadata": {
        "id": "4C8eDrtof0rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(input_ids, token_type_ids, attention_masks, labels, batch_size):\n",
        "    t_inputs = torch.tensor(input_ids)\n",
        "    t_tts = torch.tensor(token_type_ids)\n",
        "    t_masks = torch.tensor(attention_masks)\n",
        "    t_labels = torch.tensor(labels)\n",
        "\n",
        "    t_dataset = TensorDataset(t_inputs, t_tts, t_masks, t_labels)\n",
        "    batches = DataLoader(t_dataset, shuffle=True, batch_size=batch_size)\n",
        "    \n",
        "    return batches"
      ],
      "metadata": {
        "id": "ceTf4kPwflqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_batches = create_batches(input_ids, token_type_ids, attention_masks, y, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "wmP6I4efbpbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_batches)"
      ],
      "metadata": {
        "id": "H5gv5y6yf6V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Model"
      ],
      "metadata": {
        "id": "mlgBaZ7bgLLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can build the actual BERT model. This class is essentially a convenient wrapper for the DistilBERT model that we will be using from HuggingFace. It exposes a train function, into which we can pass in our training batches (and validation batches). Once it is trained, we can pass our test batch into the predict function to generate a set of predictions on the test set.\n",
        "\n",
        "The DistilBERT model will be passed into the constructor (as inner_model) along with an optimizer and device specification. \n",
        "\n"
      ],
      "metadata": {
        "id": "yrPE6kK3gMTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertModel():\n",
        "    def __init__(self, inner_model, device, optimizer):\n",
        "        self.__inner_model = inner_model\n",
        "        self.__device = device\n",
        "        self.__optimizer = optimizer\n",
        "        \n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs):\n",
        "        for i in range(epochs):\n",
        "            self._run_training_epoch(i, train_loader, val_loader)\n",
        "\n",
        "\n",
        "    def predict(self, test_loader):\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "        y_logits = []\n",
        "\n",
        "        self.__inner_model.eval()\n",
        "\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            _, next_preds, next_labels, next_logits = self._run_batch(i, batch, use_grad=False)\n",
        "            y_pred.extend(next_preds.to('cpu').numpy())\n",
        "            y_true.extend(next_labels.to('cpu').numpy())\n",
        "            y_logits.extend(next_logits.to('cpu').numpy())\n",
        "\n",
        "        return y_pred, y_true, y_logits\n",
        "\n",
        "\n",
        "    def _run_training_epoch(self, epoch, train_loader, val_loader):\n",
        "        print(f'\\n\\n --- Epoch {epoch+1}')\n",
        "\n",
        "        # Set to train mode: Tells model to compute gradients \n",
        "        self.__inner_model.train()\n",
        "        train_loss, train_acc = self._run_training_step(train_loader, use_grad=True)\n",
        "\n",
        "        # Set to eval mode\n",
        "        self.__inner_model.eval()\n",
        "        val_loss, val_acc = self._run_training_step(val_loader, use_grad=False)\n",
        "\n",
        "        # Display results\n",
        "        print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "    # Can be for either training or validation mode\n",
        "    def _run_training_step(self, data_loader, use_grad):\n",
        "        total_loss = 0\n",
        "        total_acc  = 0\n",
        "\n",
        "        for i, batch in enumerate(data_loader):\n",
        "            next_loss, next_preds, next_labels, _ = self._run_batch(i, batch, use_grad=use_grad)\n",
        "            total_loss += next_loss\n",
        "            total_acc += self._get_acc(next_preds, next_labels)\n",
        "        \n",
        "        loss = total_loss / len(data_loader)\n",
        "        acc = total_acc / len(data_loader)\n",
        "        return loss, acc\n",
        "\n",
        "\n",
        "\n",
        "    # Used for training, validation, and prediction\n",
        "    def _run_batch(self, i, batch, use_grad):\n",
        "        # Add batch to device\n",
        "        batch = tuple(t.to(self.__device) for t in batch)\n",
        "\n",
        "        # Unpack inputs from the dataloader\n",
        "        b_inputs, b_tts, b_masks, b_labels = batch\n",
        "\n",
        "        # Clear gradients\n",
        "        self.__optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(use_grad):\n",
        "            loss, logits = self.__inner_model(b_inputs, \n",
        "                                        #token_type_ids=b_tts, # Not needed for DistilBERT\n",
        "                                        attention_mask=b_masks, \n",
        "                                        labels=b_labels).values()\n",
        "        \n",
        "        preds = self._logits_to_preds(logits)\n",
        "\n",
        "        if use_grad:\n",
        "            loss.backward()\n",
        "            self.__optimizer.step()\n",
        "        \n",
        "        if i%500==0:\n",
        "            print(f'...batch {i}')\n",
        "\n",
        "        return loss.item(), preds, b_labels, logits\n",
        "\n",
        "\n",
        "    def _logits_to_preds(self, logits):\n",
        "        _sm = torch.log_softmax(logits, dim=1)\n",
        "        _argmax = _sm.argmax(dim=1)\n",
        "        return _argmax\n",
        "\n",
        "\n",
        "    def _get_acc(self, y_pred, y_true):\n",
        "        _comp = (y_pred == y_true)\n",
        "        _sum = _comp.sum().float()\n",
        "        _size = float(y_true.size(0))\n",
        "        return _sum / _size\n",
        "    "
      ],
      "metadata": {
        "id": "nnBfYYD6gImO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "aI7db-DthNVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use our BertModel class, we need to define the building blocks used in the constructor. This includes the inner DistilBERT model from HuggingFace, and optimizer, and the device.\n",
        "\n",
        "For the device, we want to use a GPU (cuda:0) if available, and this is strongly recommended in order to reduce training time. We will use a standard AdamW optimizer.\n",
        "\n",
        "For the inner model, we use DistilBERT for Sequence Classification. We specify that we want to use distilbert-base-uncased, and that there are 2 labels (paraphrase or not-paraphrase)\n",
        "\n"
      ],
      "metadata": {
        "id": "tMpJ2Y3hhO4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "4yd1S3plhHpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inner_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "inner_model.to(device)"
      ],
      "metadata": {
        "id": "gYWFx6jrhoTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_optimizer = list(inner_model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "    'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "    'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "# This variable contains all of the hyperparameter information our training loop needs\n",
        "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=2e-5)\n"
      ],
      "metadata": {
        "id": "MGKB7KnehenZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel(inner_model, device, optimizer)"
      ],
      "metadata": {
        "id": "xnQFxGGQhpQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training, recall that BERT requires a training AND a validation set. We have our training batches ready to go. We must apply the same preprocessing to our validation set. We can also do our test set, which will be needed for evaluation"
      ],
      "metadata": {
        "id": "P6Y-Kgq4iDW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val, y_val = format_df(val_df)\n",
        "i_val, t_val, m_val = get_inputs(X_val, TOKENIZER, MAX_LEN)\n",
        "val_batches = create_batches(i_val, t_val, m_val, y_val, BATCH_SIZE) "
      ],
      "metadata": {
        "id": "zxivpuWTh3xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = format_df(test_df)\n",
        "i_test, t_test, m_test = get_inputs(X_test, TOKENIZER, MAX_LEN)\n",
        "test_batches = create_batches(i_test, t_test, m_test, y_test, BATCH_SIZE) "
      ],
      "metadata": {
        "id": "QgDFiz_bjP1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train. This can take a while, especially if you are not using a GPU. We will train for 2 epochs"
      ],
      "metadata": {
        "id": "2l4FbwphjY8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(training_batches, val_batches, 2)"
      ],
      "metadata": {
        "id": "czIcO_UgjXMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test set and Evaluation"
      ],
      "metadata": {
        "id": "2VymffhEjoWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can run it against our test set and evaluate the performance"
      ],
      "metadata": {
        "id": "GYxQo6B4jqf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred, y_true, _ = model.predict(test_batches)"
      ],
      "metadata": {
        "id": "UTJOEGhsjlui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(metrics.classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "qbFtjj3gj2ZD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}