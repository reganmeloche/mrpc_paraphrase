{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Pdi_8itcweyo",
        "rdT6LU8axUD0",
        "UuE1j0Qp9Lro",
        "TXlMHY4k9RTb",
        "6UVf07vi9Tlr"
      ],
      "mount_file_id": "1N0LbINF4yMAFxGj_z_riHAH0BSJ8vDSR",
      "authorship_tag": "ABX9TyMAJMkVYf+pFIDU1K+/+gTK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reganmeloche/mrpc_paraphrase/blob/main/baseline_approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Paraphrasing - baseline approach\n"
      ],
      "metadata": {
        "id": "OgXhXPH5wOd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this approach, we will be using some traditional NLP techniques, which will give us a baseline set of results against which we can compare future approaches."
      ],
      "metadata": {
        "id": "uLqIZk4vwRbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "Pdi_8itcweyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv"
      ],
      "metadata": {
        "id": "ZJgLs1ifwQyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_PATH = '/content/drive/MyDrive/Colab Notebooks/NLP/ms_paraphrase'"
      ],
      "metadata": {
        "id": "oPYnm2ozwsgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = f'{ROOT_PATH}/data'\n",
        "\n",
        "train_df = pd.read_csv(f'{data_path}/train_df.csv')\n",
        "test_df = pd.read_csv(f'{data_path}/test_df.csv')"
      ],
      "metadata": {
        "id": "hkMqlOc3wsxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "id": "VyHz78V2wu-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "ZTTQ2Awsxk3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to be taking linguistic properties from the text, but first we want to perform some basic preprocessing: tokenization, stop-word removal, lemmatization, normalization, etc."
      ],
      "metadata": {
        "id": "T6ZKO4xmxmkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "4ECIr6kSypHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "6XtsG8fyyDFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "nlp.remove_pipe('ner')\n",
        "nlp.remove_pipe('attribute_ruler')\n",
        "print(nlp.pipe_names)"
      ],
      "metadata": {
        "id": "syQ5Fxb_zul-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a preprocessing function that we will apply to all of the sentences."
      ],
      "metadata": {
        "id": "2_Pn534dzReU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = nlp.Defaults.stop_words\n",
        "\n",
        "def preprocess(text):\n",
        "    # Text pre-processing\n",
        "\n",
        "    # Lowercase it all\n",
        "    text = text.lower()\n",
        "\n",
        "    # Replace dash with space\n",
        "    text = text.replace(\"-\", \" \")\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = ''.join(c for c in text if c not in punctuation)\n",
        "\n",
        "    # Replace digits with standard\n",
        "    text = re.sub(r'\\d+', '#', text)\n",
        "\n",
        "    # Spacy preprocessing\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Lemmatize\n",
        "    tokens = [t.lemma_ for t in doc]\n",
        "\n",
        "    # Remove stop words\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "vqCxHvLzzhcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = 'I took the 3 dogs for a walk last Tuesday at 8pm!'\n",
        "sample_prepro = preprocess(sample_text)\n",
        "print(sample_prepro)"
      ],
      "metadata": {
        "id": "D6QgiRHV1aAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we apply our preprocessing to our data. The preprocessing step may take a few minutes to run"
      ],
      "metadata": {
        "id": "uAgx-2M-zlSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df[['s1','s2']].values\n",
        "y = train_df['label'].values"
      ],
      "metadata": {
        "id": "h3PY4P2k1xxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xp = [[preprocess(x[0]), preprocess(x[1])] for x in X]"
      ],
      "metadata": {
        "id": "rCmyRWea3zHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorizing"
      ],
      "metadata": {
        "id": "rdT6LU8axUD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use a standard TFIDF approach for our baseline. This involves creating a corpus out of all of our preprocessed sentences and fitting a TFIDF vectorizer to that corpus. Any sentence in our training corpus will then map to a TFIDF vector"
      ],
      "metadata": {
        "id": "EaPw8ILNxVok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "\n",
        "for x in Xp:\n",
        "    corpus.append(' '.join(x[0]))\n",
        "    corpus.append(' '.join(x[1]))"
      ],
      "metadata": {
        "id": "Xs1JWXrR5uCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)"
      ],
      "metadata": {
        "id": "-dxN_Z2Sw31O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = [' '.join(x[0]) for x in Xp]\n",
        "X2 = [' '.join(x[1]) for x in Xp]\n",
        "\n",
        "print(X1[3])"
      ],
      "metadata": {
        "id": "KZlLyTWF9Hei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1t = vectorizer.transform(X1)\n",
        "X2t = vectorizer.transform(X2)"
      ],
      "metadata": {
        "id": "ZhCcZ1-I04Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distance measurement"
      ],
      "metadata": {
        "id": "UuE1j0Qp9Lro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have all of our sentences preprocessed and transformed to TFIDF vectors.\n",
        "\n",
        "We can now use a similarity measurment to get a sense of how \"close\" each sentence is to it's paired partner. We use the cosine similarity distance measurement from the scipy library.\n"
      ],
      "metadata": {
        "id": "HSzK6JDM1bNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "\n",
        "def get_dist(x1, x2):\n",
        "    a = x1.toarray()\n",
        "    b = x2.toarray()\n",
        "    return 1 - spatial.distance.cosine(a, b)\n"
      ],
      "metadata": {
        "id": "A97f0Y6D1p4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dist = get_dist(X1t[0], X2t[0])\n",
        "print(test_dist)"
      ],
      "metadata": {
        "id": "lAwtjzl4412C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can come up with a threshold by calculating the average distance for all of the pairs that have a label of 1."
      ],
      "metadata": {
        "id": "UGo3Zcse4ORN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get all distances\n",
        "distances = [get_dist(x1,x2) for (x1,x2) in zip(X1t, X2t)]\n",
        "\n",
        "# Filter to keep only those that are labeled as a paraphrase\n",
        "matches = [d for i,d in enumerate(distances) if y[i] == 1]\n",
        "\n",
        "# calculate the average\n",
        "threshold = np.average(matches)\n",
        "\n",
        "print(threshold)"
      ],
      "metadata": {
        "id": "vnlQfOLV4UP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test set"
      ],
      "metadata": {
        "id": "TXlMHY4k9RTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can apply the same treatment to our test cases and then use the threshold to predict if they are a paraphrase.\n",
        "\n",
        "First we perform the regular processing"
      ],
      "metadata": {
        "id": "kVHu21nx6ues"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_df[['s1','s2']].values\n",
        "y_test = test_df['label'].values\n",
        "\n",
        "Xp_test = [[preprocess(x[0]), preprocess(x[1])] for x in X_test]\n",
        "\n",
        "X1_test = [' '.join(x[0]) for x in Xp_test]\n",
        "X2_test = [' '.join(x[1]) for x in Xp_test]\n",
        "\n",
        "X1t_test = vectorizer.transform(X1_test)\n",
        "X2t_test = vectorizer.transform(X2_test)"
      ],
      "metadata": {
        "id": "0dcSnlox6tol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we calculate the distances"
      ],
      "metadata": {
        "id": "YNNj1uMk7uJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_distances = [get_dist(x1,x2) for (x1,x2) in zip(X1t_test, X2t_test)]"
      ],
      "metadata": {
        "id": "VrXwVNab7teO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for each distance, if it is over our threshold we predict a 1 (the pair of sentences IS a paraphrase), otherwise we predict a 0 (not a paraphrase)"
      ],
      "metadata": {
        "id": "bqoCFzRu7wF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x):\n",
        "    if x > threshold:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "0CQhDMmP7_Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = [predict(x) for x in test_distances]"
      ],
      "metadata": {
        "id": "QPpkPQLx7oSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "6UVf07vi9Tlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we compare our predictions against the actual test set to see how well our baseline classifier performed"
      ],
      "metadata": {
        "id": "2PDXde7D8PQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "oXxawssd8YAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is unbalanced so we're more interested in the precision, recall, and f-score than we are in the accuracy. The results are quite weak, so it shouldn't be too difficult to improve on this baseline."
      ],
      "metadata": {
        "id": "r3v7ceej8vqG"
      }
    }
  ]
}